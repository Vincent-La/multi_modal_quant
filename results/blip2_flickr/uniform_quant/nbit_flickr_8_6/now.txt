python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12 --qformer-layer-indices 0,1,2,3 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25 --qformer-layer-indices 0,1,2,3,4,5,6,7 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 8 --qformer-self-attention-weight-bits 8 --qformer-cross-attention-weight-bits 8 --qformer-img-ff-weight-bits 8 --qformer-text-ff-weight-bits 8 --visual-encoder-block-modules qkv,proj,fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules fc1,fc2
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-self-attention-modules query,key,value,dense --qformer-cross-attention-modules query,key,value,dense
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-text-ff-modules intermediate,output
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2 --qformer-img-ff-modules intermediate_query,output_query
python -m torch.distributed.run --nproc_per_node=8 evaluate.py --cfg-path ret_flickr_eval.yaml --visual-encoder-block-indices 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38 --qformer-layer-indices 0,1,2,3,4,5,6,7,8,9,10,11 --visual-encoder-block-weight-bits 6 --qformer-self-attention-weight-bits 6 --qformer-cross-attention-weight-bits 6 --qformer-img-ff-weight-bits 6 --qformer-text-ff-weight-bits 6 --visual-encoder-block-modules qkv,proj,fc1,fc2
